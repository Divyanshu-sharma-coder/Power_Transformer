Power Transformer: Box-Cox & Yeo-Johnson with Linear Regression

ðŸ“Œ Overview

This project demonstrates the impact of data transformation techniques on model performance.
Using Box-Cox and Yeo-Johnson transformations, I improved the predictive performance of a Linear Regression model, with each transformation resulting in a higher RÂ² score compared to the baseline.


---

ðŸš€ Features

Box-Cox Transformation: Applied to stabilize variance and make data more normal distribution-like.

Yeo-Johnson Transformation: Similar to Box-Cox but works with both positive and negative values.

Linear Regression Model: Implemented after each transformation to track performance improvement.

Performance Tracking: RÂ² score calculated for:

Original data (baseline)

After Box-Cox transformation

After Yeo-Johnson transformation




---

ðŸ“Š Results

Transformation	RÂ² Score

Original Data	baseline score
Box-Cox	score after Box-Cox
Yeo-Johnson	score after Yeo-Johnson


> âœ… In each case, the RÂ² score increased, showing that proper transformations can significantly improve model accuracy.




---

ðŸ›  Tech Stack

Python

Pandas

NumPy

Scikit-learn

Matplotlib (for visualization)



---

ðŸ“‚ Repository Structure

Power_Transformer/
â”‚â”€â”€ Power_Transformer.ipynb     # Main notebook with transformations & model
â”‚â”€â”€ README.md                   # Project documentation
â”‚â”€â”€ requirements.txt            # Python dependencies


---

ðŸ“– How to Run

1. Clone the repository:

git clone https://github.com/Divyanshu-sharma-coder/Power_Transformer.git
cd Power_Transformer


2. Install dependencies:

pip install -r requirements.txt


3. Run the notebook:

Open Power_Transformer.ipynb in Jupyter Notebook or Google Colab.

Execute cells in order.





---

ðŸŽ¯ Key Takeaways

Data transformations like Box-Cox and Yeo-Johnson can boost model performance.

Always check RÂ² score (or other metrics) before and after transformations.

Simple preprocessing can lead to significant accuracy 
